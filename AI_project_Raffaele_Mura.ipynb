{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Artificial Intelligence project\n",
        "\n",
        "\n",
        "### Raffaele Mura 70/90/00312\n",
        "A.Y 2022/2023"
      ],
      "metadata": {
        "id": "yiz0B3almIz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automated text categorization, or text classification, has gained significant attention in the last decade due to the growing volume of digital text, advancements in machine learning algorithms, and its wide-ranging applications in information retrieval, sentiment analysis, customer support, healthcare, and marketing, among others. It has become a crucial tool for organizing and making sense of the vast amount of unstructured textual data available online and has enabled the development of more accurate and efficient solutions in various industries.\n",
        "\n",
        "This project aims to explore one of the main approaches to text categorization thorugh machine learning tools. The specific objective is to categorize textual news data from the Reuters news agency into predefined categories that describe their content.\n",
        "\n",
        "The problem is a multi-label classification task, where each news document can belong to one or more categories from a predefined set of approximately 90 categories. The goal is to train a multi-label classifier using the \"one-vs-all\" approach, where *M* binary classifiers are independently trained for each category, and then the categories assigned by these classifiers are combined to form the final label set for each document.\n",
        "\n",
        "In this Python code, we will employ various techniques to perform data preprocessing and evaluation to develop an efficient multi-label text categorization model for the Reuters news dataset. The code will encompass data loading, preprocessing, feature selection, classifier training, and model evaluation.\n",
        "\n",
        "To achieve this, the initial steps of data preprocessing involve selecting informative features associated with terms present in the documents. The standard approach is to evaluate the discriminatory power of each term present in the training set and select the *N* most discriminative terms. The documents are then represented as feature vectors containing a relevant representation of the selected *N* terms.\n",
        "\n",
        "Before feature selection, it is essential to eliminate \"stop words,\" which are non-discriminatory terms like articles and prepositions, and perform lemmatization (stemming) to reduce the number of features to be evaluated. Lemmatization reduces each word to its base form, e.g., transforming various forms of a single verb (e.g., make, made, making) to a common root like \"mak.\"\n",
        "\n",
        "Finally, in the evaluation phase, the performance of the multi-label classifier will be assessed using the \\\\(F_{Î²}\\\\) measure, a combination of precision and recall.\n",
        "\n"
      ],
      "metadata": {
        "id": "vNvwgYrCmvam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the Dataset"
      ],
      "metadata": {
        "id": "s0JnrAtU8x3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this first section there is the code related to the download of the dataset."
      ],
      "metadata": {
        "id": "O5FXAXsUqgF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os.path\n",
        "import tarfile\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "qL9P1J5-9zzX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for progress bar\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"<progress value='{value}' max='{max}', style='width: 100%'>\n",
        "            {value}\n",
        "        </progress>\"\"\".format(value=value, max=max))\n"
      ],
      "metadata": {
        "id": "2npw2BTVFTBM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "url = 'http://www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz'\n",
        "# This requests the resource at the given link, extracts its contents\n",
        "# and saves it in a variable\n",
        "data = requests.get(url).content\n",
        "f = open('reuters21578.tar.gz','wb')\n",
        "# Storing the dataset inside the data variable to the file\n",
        "f.write(data)\n",
        "f.close()\n",
        "#extracting the files inside the archive\n",
        "file = tarfile.open('/content/reuters21578.tar.gz')\n",
        "file.extractall('./dataset')\n",
        "file.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "a8Fx1TLK8w9U"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting data from the dataset"
      ],
      "metadata": {
        "id": "JplJiqah-5uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions below are used for a first handling of the data in the Reuters dataset.\n",
        "The dataset is distributed in 22 files, where the first 21 files contain 1000 documents, while the last contains 578 documents.\n",
        "All files are in SGML format, and all of them contain some tags used to describe the documents in each file.\n",
        "\n",
        "Some examples of tags are 'TOPICS', which refer to the categories of the document, or the tag 'BODY' which contain the main text of each document.\n",
        "\n",
        "the **minor_preprocess** function takes a file path as input, reads the file, and performs minor preprocessing steps to clean the data. It first opens the file in binary mode, reads its lines, and converts them to utf-8 safe lines, ignoring any characters that cannot be decoded. Then, it removes problematic strings containing numeric character references (e.g., &#123;). Finally, it replaces newline characters with spaces. The processed lines are joined to form a single string, which is returned."
      ],
      "metadata": {
        "id": "rK_GmBMir3Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def minor_preprocess(file):\n",
        "    with open(file, 'rb') as f:\n",
        "        lines = f.readlines()\n",
        "        utf8_safe_lines = [line.decode('utf-8', 'ignore') for line in lines]\n",
        "        xml_safe_lines = [re.sub(r'&#\\d*;', '', line) for line in utf8_safe_lines]  # Get rid of problematic strings\n",
        "        no_newlines = [line.replace('\\n', ' ') for line in xml_safe_lines]\n",
        "    f.close()\n",
        "\n",
        "    return ''.join(no_newlines)"
      ],
      "metadata": {
        "id": "hw4EEvku--hp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **compile_data** function compiles data from multiple files within a specified datapath directory. It iterates through all files in the directory and selects those with the '.sgm' extension. For each selected file, it calls the minor_preprocess function to preprocess its content. The preprocessed data is then split into individual 'REUTERS' records, and these records are concatenated into a single list called dataset. The function returns this list containing all the preprocessed records."
      ],
      "metadata": {
        "id": "EVLfrUhzwQZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_data(datapath='./dataset'):\n",
        "    dataset = []\n",
        "    for file in os.listdir(datapath):\n",
        "        if file.endswith('.sgm'):\n",
        "            preprocessed_data = minor_preprocess(datapath + '/' + file)\n",
        "            records = [record + '</REUTERS>' for record in preprocessed_data.split('</REUTERS>') if\n",
        "                       record]  # Retain all original formatting\n",
        "            dataset.extend(records)\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "RgDTOM-cwSU_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **compile_dictionary** function takes a single 'REUTERS' record (as a string) as input. It initializes a dictionary called data_dict with keys corresponding to various fields (the tags of the document) of the record, such as 'REUTERS TOPICS', 'LEWISSPLIT', 'TOPICS', 'TITLE', and 'BODY'. It then extracts information from the 'data' string using string manipulations and the BeautifulSoup library (used for parsing XML data).\n",
        "\n",
        "*   The 'REUTERS TOPICS' tag could contain *YES* or *NO* if the document has an\n",
        "assigned topic or not.\n",
        "*   The 'LEWISSPLIT' tag contains *TRAIN* or *TEST* to indicate how the documents wshould be splitted in the training or testing dataset\n",
        "*   Finally the tags 'TOPICS', 'TITLE' and 'BODY' contains respectively the category, the title and the main text of the document.  \n",
        "\n",
        "After retriving this data the function returns data_dict, a dictionary containing the extracted information from the 'REUTERS' record.\n",
        "\n"
      ],
      "metadata": {
        "id": "z4lKGp1ZwWhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_dictionary(data):\n",
        "    data_dict = {\n",
        "        'REUTERS TOPICS': '',\n",
        "        'LEWISSPLIT': '',\n",
        "        'TOPICS': 'none',\n",
        "        'TITLE': '',\n",
        "        'BODY': '',\n",
        "    }\n",
        "\n",
        "    # Grab the Reuters Topics between the following tags\n",
        "    start = data.find('<REUTERS TOPICS=\"') + len('<REUTERS TOPICS=\"')\n",
        "    end = data.find('\" LEWISSPLIT=')\n",
        "    data_dict['REUTERS TOPICS'] = data[start:end]\n",
        "    start = data.find('LEWISSPLIT=\"') + len('LEWISSPLIT=\"')\n",
        "    end = data.find('\" CGISPLIT=')\n",
        "    data_dict['LEWISSPLIT'] = data[start:end]\n",
        "\n",
        "    soup = BeautifulSoup(data, 'xml')\n",
        "\n",
        "    # Use a try/except block to grab Topics, Title, and Body in case they are empty\n",
        "    # If empty, the default value remains unchanged\n",
        "    try:\n",
        "        if soup.TOPICS.contents:\n",
        "            data_dict['TOPICS'] = soup.TOPICS.contents[:]\n",
        "            data_dict['TOPICS'] = [str(data).replace('<D>', '') for data in data_dict['TOPICS']]\n",
        "            data_dict['TOPICS'] = [str(data).replace('</D>', '') for data in data_dict['TOPICS']]\n",
        "\n",
        "        if soup.TITLE.contents:\n",
        "            data_dict['TITLE'] = soup.TITLE.contents[0]\n",
        "\n",
        "        if soup.BODY.contents:\n",
        "            body = soup.BODY.contents[0]\n",
        "            data_dict['BODY'] = soup.BODY.contents[0]\n",
        "    except AttributeError:\n",
        "        pass\n",
        "\n",
        "    return data_dict\n"
      ],
      "metadata": {
        "id": "fNPZAj81wW2R"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the large imbalance of samples in the various categories, the code below aims to retrieve from the dataset the labels of the categories that are prevalent in the dataset.\n",
        "The definition of the variable below number_of_classes determines the number of categories to consider.\n",
        "The higher this variable is, the more the problem at hand becomes a multi-label classification problem.\n",
        "This follows from the fact that with the definition of this variable we will consider a maximum 'number_of_classes'.  And these are equivalent, as described later, to the classes most occurring over the entire dataset.\n",
        "However, this has a drawback: if a sample belongs to multiple classes, they must all be within the most common on the dataset, otherwise it will be categorized only according to the most common one.\n"
      ],
      "metadata": {
        "id": "yx_dFV3Vzz_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_classes = 20"
      ],
      "metadata": {
        "id": "sRS6pOOiAWAs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code segment below performs several actions:\n",
        "The *compile_data* function is called to create the dataset list containing preprocessed 'REUTERS' records.\n",
        "\n",
        "Then we iterate through each element in the dataset list and applies the compile_dictionary function to convert each 'REUTERS' record into a dictionary format. The resulting dictionaries are stored in the dataset_dicts list.\n",
        "Then we filter the dataset_dicts list, removing any entries that have the value 'none' for the key 'TOPICS'. This step removes any records without topics, as indicated by the 'none' value.\n",
        "\n",
        "To understand which are the most common labels we extract all the values of the 'TOPICS' key from each dictionary in the dataset_dicts list and stores them in the labels list.\n",
        "Then we will retrieve the *number_of_classes* most common labels and stores them in the most_common_labels list. The *most_common* method of the Counter object returns a list of tuples containing the label and its corresponding count, sorted in descending order of counts.\n",
        "\n",
        "Finally we filter the dataset_dicts list, keeping only the records whose 'TOPICS' value is present in the most_common_labels list. This step further reduces the dataset to only include the most common classes."
      ],
      "metadata": {
        "id": "nxwehgD91HCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = compile_data()\n",
        "dataset_dicts = [compile_dictionary(data) for data in dataset]\n",
        "dataset_dicts = [data for data in dataset_dicts if data['TOPICS'] != 'none']\n",
        "\n",
        "# maintaining the rows of the most common label\n",
        "labels = [data[\"TOPICS\"] for data in dataset_dicts]\n",
        "flat_labels = [item for sublist in labels for item in sublist]\n",
        "common_labels = Counter(flat_labels)\n",
        "most_common_labels = common_labels.most_common(number_of_classes)\n",
        "\n",
        "most_common_labels = [most_common_labels[i][0] for i in range(number_of_classes)]\n",
        "\n",
        "print('the most common labels are:', most_common_labels)\n",
        "\n",
        "with open('labels.pkl', 'wb') as f:\n",
        "  pickle.dump(most_common_labels, f)\n",
        "#here we are maintaning only the samples which have as first label a label inside the most common list\n",
        "dataset_dicts = [data for data in dataset_dicts if data['TOPICS'][0] in most_common_labels]\n",
        "\n",
        "#this cycle is used to store in the body the title of each sample which has an empty body\n",
        "for data in dataset_dicts:\n",
        "  if data['BODY'] == '':\n",
        "    data['BODY'] = data['TITLE']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrQx6K7V_pBX",
        "outputId": "c7f3cf10-10ab-4057-ae2a-b5a14a734d17"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the most common labels are: ['earn', 'acq', 'money-fx', 'crude', 'grain', 'trade', 'interest', 'wheat', 'ship', 'corn', 'dlr', 'oilseed', 'money-supply', 'sugar', 'gnp', 'coffee', 'veg-oil', 'gold', 'nat-gas', 'soybean']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-Processing of data\n",
        "The next code segment uses the Natural Language Toolkit (NLTK) library to perform text preprocessing on the previously retrieved text data.\n",
        "The main tasks carried out by the functions are:\n",
        "*   the **removing_stop_words** function takes a pandas DataFrame as input and removes stop words from the text data in the \"TITLE\" and \"BODY\" columns of the DataFrame. The function iterates through the rows of the DataFrame using the DataFrame index. For each row, it splits the text in \"TITLE\" and \"BODY\" columns into individual words using whitespace as the separator. Then, it checks each word against the set of English stop words obtained from NLTK. If the word is not a stop word, it appends it to the new version of the title and body texts, effectively removing stop words from the texts. The updated title and body texts are then stored back into the DataFrame.\n",
        "\n",
        "*   the **stemming** function takes the DataFrame as input and applies stemming to the text data in the \"TITLE\" and \"BODY\" columns. Stemming is a process of reducing words to their base or root form (e.g., \"running\" to \"run\" or \"jumps\" to \"jump\"). The function initializes a PorterStemmer object from NLTK. It then iterates through the rows of the DataFrame using the DataFrame index. For each row, it tokenizes the text in the \"TITLE\" and \"BODY\" columns into individual words using NLTK's word_tokenize method. It applies stemming to each word using the PorterStemmer, and then it reconstructs the new version of the title and body texts with the stemmed words."
      ],
      "metadata": {
        "id": "QY1mVWjv4JDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "def removing_stop_words(dataframe):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for i in dataframe.index:\n",
        "\n",
        "        new_title = ''\n",
        "        new_body = ''\n",
        "\n",
        "        title_text = dataframe[\"TITLE\"][i]\n",
        "        title_words = title_text.split()\n",
        "        body_text = dataframe[\"BODY\"][i]\n",
        "        body_words = body_text.split()\n",
        "\n",
        "        for t in title_words:\n",
        "            if not t in stop_words:\n",
        "                new_title = new_title + ' ' + t\n",
        "\n",
        "        for b in body_words:\n",
        "            if not b in stop_words:\n",
        "                new_body = new_body + ' ' + b\n",
        "\n",
        "        dataframe[\"TITLE\"][i] = new_title\n",
        "        dataframe[\"BODY\"][i] = new_body\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "def stemming(dataframe):\n",
        "    ps = PorterStemmer()\n",
        "    for i in dataframe.index:\n",
        "\n",
        "        new_title = ''\n",
        "        new_body = ''\n",
        "\n",
        "        title_text = dataframe[\"TITLE\"][i]\n",
        "        title_words = word_tokenize(title_text)\n",
        "\n",
        "        body_text = dataframe[\"BODY\"][i]\n",
        "        body_words = word_tokenize(body_text)\n",
        "        for t in title_words:\n",
        "            new_title = new_title + ' ' + ps.stem(t)\n",
        "        for b in body_words:\n",
        "            new_body = new_body + ' ' + ps.stem(b)\n",
        "\n",
        "        dataframe[\"TITLE\"][i] = new_title\n",
        "        dataframe[\"BODY\"][i] = new_body\n",
        "\n",
        "    return dataframe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVh4-ytAAMqa",
        "outputId": "e6a3511d-e32f-4e7c-8346-a6cbb603dfd6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below creates a pandas dataframe from the dictionary implmented before and the calls the two function previously described on it.\n",
        "Finally it stores the dataframe as a pickle file.\n"
      ],
      "metadata": {
        "id": "wdMshdLP5e2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dataset_dicts)\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n after the cleaning of the data \\n\")\n",
        "df = removing_stop_words(df)\n",
        "df = stemming(df)\n",
        "print(df.head())\n",
        "df.to_pickle('dataframe.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rIOE64IAMer",
        "outputId": "ae4d14e9-f626-4e25-9d8d-4220e55521e0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  REUTERS TOPICS LEWISSPLIT                TOPICS  \\\n",
            "0            YES      TRAIN                [earn]   \n",
            "1            YES      TRAIN                [earn]   \n",
            "2            YES      TRAIN  [money-fx, interest]   \n",
            "3            YES      TRAIN                [earn]   \n",
            "4            YES      TRAIN                [earn]   \n",
            "\n",
            "                                              TITLE  \\\n",
            "0              NATIONAL FSI INC <NFSI> 4TH QTR LOSS   \n",
            "1      <PRECAMBRIAN SHIELD RESOURCES LTD> YEAR LOSS   \n",
            "2  U.K. MONEY MARKET GIVEN FURTHER 437 MLN STG HELP   \n",
            "3     GREASE MONKEY HOLDING CORP <GMHC> YEAR NOV 30   \n",
            "4     ACCEPTANCE INSURANCE HOLDINGS INC <ACPT> YEAR   \n",
            "\n",
            "                                                BODY  \n",
            "0  Shr loss six cts vs profit 19 cts     Net loss...  \n",
            "1  Shr loss 1.93 dlrs vs profit 16 cts     Net lo...  \n",
            "2  The Bank of England said it had provided the m...  \n",
            "3  Shr nil vs nil     Net 130,998 vs 30,732     R...  \n",
            "4  Oper shr profit 1.80 dlrs vs loss 2.28 dlrs   ...  \n",
            "\n",
            " after the cleaning of the data \n",
            "\n",
            "  REUTERS TOPICS LEWISSPLIT                TOPICS  \\\n",
            "0            YES      TRAIN                [earn]   \n",
            "1            YES      TRAIN                [earn]   \n",
            "2            YES      TRAIN  [money-fx, interest]   \n",
            "3            YES      TRAIN                [earn]   \n",
            "4            YES      TRAIN                [earn]   \n",
            "\n",
            "                                               TITLE  \\\n",
            "0               nation fsi inc < nfsi > 4th qtr loss   \n",
            "1       < precambrian shield resourc ltd > year loss   \n",
            "2   u.k. money market given further 437 mln stg help   \n",
            "3        greas monkey hold corp < gmhc > year nov 30   \n",
            "4                accept insur hold inc < acpt > year   \n",
            "\n",
            "                                                BODY  \n",
            "0   shr loss six ct vs profit 19 ct net loss 166,...  \n",
            "1   shr loss 1.93 dlr vs profit 16 ct net loss 53...  \n",
            "2   the bank england said provid money market 437...  \n",
            "3   shr nil vs nil net 130,998 vs 30,732 rev 1,56...  \n",
            "4   oper shr profit 1.80 dlr vs loss 2.28 dlr ope...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right above we can see the print of the head of the dataset before and after the removing of stop words and stemming operations.\n",
        "The results of these operations are visible in the BODY section of each record, where there is the absence of stop words and and there are only base words.\n"
      ],
      "metadata": {
        "id": "eDQreIV3Gep2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Test Split\n",
        "\n",
        "The function below is used to split the DataFrame into training and testing sets. The data is read from the pickle file then performs the following steps:\n",
        "\n",
        "\n",
        "\n",
        "*   It initializes empty lists: _train, y_train, X_test, and y_test, which will be used to store the training and testing data.\n",
        "*   It iterates through the rows of the DataFrame using its index, and for each row if the value of 'LEWISSPLIT' in that row is 'TRAIN', it appends the 'BODY' value to X_train and the 'TOPICS' value to y_train. If the value of 'LEWISSPLIT' in that row is 'TEST', it appends the 'BODY' value to X_test and the 'TOPICS' value to y_test.\n",
        "\n",
        "Before saving the arrays related to the targets, we use the *MultiLabelBinarizer* to convert the multi-label targets into a binary format, suitable for multi-label classification tasks. It also encodes the labels using one-hot encoding.\n",
        "We are passing to the MultiLabelBinarizer the parameter 'labels', which corresponds to the set of mosto common label that we are considering.\n",
        "This results in an encoding only considering the labels included in the most common list."
      ],
      "metadata": {
        "id": "DETaCpNoA7h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "\n",
        "def train_test_split(labels):\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "    df = pd.read_pickle('dataframe.pkl')\n",
        "    for i in df.index:\n",
        "\n",
        "        if df['LEWISSPLIT'][i] == 'TRAIN':\n",
        "            X_train.append(df['BODY'][i])\n",
        "            y_train.append(df['TOPICS'][i])\n",
        "        elif df['LEWISSPLIT'][i] == 'TEST':\n",
        "            X_test.append(df['BODY'][i])\n",
        "            y_test.append(df['TOPICS'][i])\n",
        "\n",
        "    print('len x train {}, len y train {}, len x test {}, len y test {}'.format(len(X_train),\n",
        "                                                                                len(y_train),\n",
        "                                                                                len(X_test),\n",
        "                                                                                len(y_test)))\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_train = np.array(y_train, dtype='object')\n",
        "    y_test = np.array(y_test, dtype='object')\n",
        "\n",
        "    mlb = MultiLabelBinarizer(classes=labels)\n",
        "    y_train = mlb.fit_transform(y_train)\n",
        "    y_test = mlb.fit_transform(y_test)\n",
        "\n",
        "    with open('X_train.pkl', 'wb') as f:\n",
        "        pickle.dump(X_train, f)\n",
        "    with open('X_test.pkl', 'wb') as f:\n",
        "        pickle.dump(X_test, f)\n",
        "    with open('y_train.pkl', 'wb') as f:\n",
        "        pickle.dump(y_train, f)\n",
        "    with open('y_test.pkl', 'wb') as f:\n",
        "        pickle.dump(y_test, f)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n"
      ],
      "metadata": {
        "id": "Nz87-k7gA5m4"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "with open('labels.pkl', 'rb') as f:\n",
        "    labels = pickle.load(f)\n",
        "X_train, X_test, y_train, y_test = train_test_split(labels)\n",
        "\n",
        "print(y_train[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp_TEdpNTn0z",
        "outputId": "c859dc38-e0bc-43bf-da49-662e382e537e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len x train 7046, len y train 7046, len x test 2713, len y test 2713\n",
            "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see above, the encoding of the targets result in a matrix where for each row we have all the all the labels which pertains to a sample. Each entry equal to one in the matrix means the appartenence of the i-th sample to those labels.\n",
        "Now, considering the 20 most common classes, only few sample pertain to more than one classes which are included in the most commons."
      ],
      "metadata": {
        "id": "GCfHdmxDWFWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Selection\n",
        "The code segment below performs feature selection and computes the Term Frequency-Inverse Document Frequency (TF-IDF) representation of the most discriminant words in the input text data.\n",
        "\n",
        "First we set the *number_of_features*  to be considered as discriminant. This value has been decided after a trial and error approach. By changing it and executing the sections below we can see how performances change.\n",
        "\n",
        "The first function **retrieving_most_discriminant_words** performs a feature selection using mutual information between the text features (X_train) and the corresponding target labels (y_train).\n",
        "\n",
        "Mutual information is a concept from information theory that measures the amount of information shared between two random variables. In the context of feature selection for machine learning, mutual information is often used to quantify the amount of information one feature provides about the target variable.Moreover mutual information (MI) measures the dependency or association between two variables. For discrete random variables X and Y, the mutual information MI(X, Y) is defined as the reduction in uncertainty about X given the knowledge of Y, or vice versa. In other words, it tells us how much knowing the value of one variable would help us predict the value of the other.\n",
        "Higher mutual information indicates a stronger dependency between the feature and the target, suggesting that the feature is more informative for predicting the target variable.\n",
        "\n",
        "In order to compute MI, in the related function we will first use CountVectorizer to convert text data in a document-term matrix which represents the word occurrence counts for each document.\n",
        "Then we calculate the mutual information between each word's count in the matrix and the corresponding target labels. The mutual_info_classif function returns a dictionary with word-to-mutual-information mapping.\n",
        "Finally we sort the mutual information dictionary in descending order based on mutual information scores and keep the top *number_of_features* most discriminant words.\n",
        "\n",
        "The function **computing_tfidf** computes the TF-IDF representation of the input text data using the most discriminant words obtained from the previous step.\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical representation of text data. It measures the importance of words in a document with respect to a collection of documents. TF computes the word frequency in a document, while IDF assesses the rarity of words across the collection. The final TF-IDF score for a word is the product of its TF and IDF. High TF-IDF scores indicate words that are frequent in a document but rare in the collection, making them more informative.\n",
        "The steps in this function are:\n",
        "- Creating a TfidfVectorizer object with the vocabulary set to the most discriminant words and the maximum number of features set to number_of_features.- Converting the input text data into a TF-IDF weighted document-term matrix using the TfidfVectorizer.\n",
        "- Applying the TF-IDF transformer to normalize the TF-IDF vectors in the matrix.\n",
        "- Converting the sparse TF-IDF matrix into a dense numpy array X.\n"
      ],
      "metadata": {
        "id": "CMKq4N7nBFFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_features = 1200"
      ],
      "metadata": {
        "id": "3VY6fj3nE2Ee"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "from operator import itemgetter\n",
        "\n",
        "def retrieving_most_discriminant_words(X_train, y_train):\n",
        "  #progress bar related code\n",
        "  out = display(progress(0, y_train.shape[1]), display_id=True)\n",
        "  kk = 0\n",
        "  ##########\n",
        "  cv = CountVectorizer()\n",
        "  X_vec = cv.fit_transform(X_train)\n",
        "\n",
        "  # Compute mutual information for each label individually\n",
        "  mi_per_label = []\n",
        "  for label_idx in range(y_train.shape[1]):\n",
        "      mi = mutual_info_classif(X_vec, y_train[:, label_idx], discrete_features=True, n_neighbors=3)\n",
        "      mi_per_label.append(mi)\n",
        "\n",
        "      #################\n",
        "      kk=kk+1\n",
        "      out.update(progress(kk, y_train.shape[1]))\n",
        "\n",
        "  # Calculate the average mutual information across all labels\n",
        "  average_mi = np.mean(mi_per_label, axis=0)\n",
        "\n",
        "  IG = dict(zip(cv.get_feature_names_out(), average_mi))\n",
        "  most_discriminant = dict(sorted(IG.items(), key=itemgetter(1), reverse=True)[:number_of_features])\n",
        "  print('Number of feature to consider:', len(most_discriminant))\n",
        "  most_discriminant_features = list(most_discriminant.keys())\n",
        "  print('the 20 most discriminant features:', most_discriminant_features[0:20])\n",
        "  with open('most_discriminant_features.pkl', 'wb') as f:\n",
        "      pickle.dump(most_discriminant_features, f)\n",
        "  return most_discriminant_features\n",
        "\n",
        "def computing_tfidf(most_discriminant_features, X, print=False):\n",
        "\n",
        "    cv = TfidfVectorizer(max_features=number_of_features, vocabulary=most_discriminant_features)\n",
        "    X_vec = cv.fit_transform(X)\n",
        "\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    X_tfidf = tfidf_transformer.fit_transform(X_vec)\n",
        "    idf = dict({'feature_name': cv.get_feature_names_out(), 'idf_weights': tfidf_transformer.idf_})\n",
        "    tf_idf = pd.DataFrame(X_tfidf.toarray(), columns=cv.get_feature_names_out())\n",
        "    if print:\n",
        "      print(tf_idf.head())\n",
        "    X = X_tfidf.toarray()\n",
        "\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "_YWCvedqA5du"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_discriminant_words = retrieving_most_discriminant_words(X_train, y_train)\n",
        "X_train = computing_tfidf(most_discriminant_words, X_train, print=True)\n",
        "X_test = computing_tfidf(most_discriminant_words, X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "hdxqr8rYA5Sa",
        "outputId": "8d9480e2-e102-4f86-9492-b3bce3a63af9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<progress value='20' max='20', style='width: 100%'>\n",
              "            20\n",
              "        </progress>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of feature to consider: 2000\n",
            "the 20 most discriminant features: ['vs', 'ct', 'shr', 'net', 'said', 'wheat', 'rev', 'tonn', 'bank', 'oil', 'export', 'trade', 'the', 'agricultur', 'loss', '000', 'inc', 'rate', 'corn', 'mln', 'dollar', 'grain', 'pct', 'soybean', 'profit', 'market', 'share', 'acquir', 'compani', 'corp', 'note', 'div', 'money', 'import', 'barrel', 'currenc', 'japan', 'countri', 'price', 'dlr', 'product', 'crop', 'offici', 'record', 'would', 'week', 'usda', 'avg', 'sugar', 'yen', 'prior', 'coffe', 'central', 'qtli', 'crude', 'billion', 'last', 'dealer', 'offer', 'govern', 'ship', 'dividend', 'qtr', 'produc', 'exchang', 'acquisit', 'econom', 'depart', 'year', 'minist', 'state', 'today', 'agreement', 'foreign', 'farmer', 'deficit', 'gold', 'stake', 'nation', 'ga', 'treasuri', 'mth', 'reserv', 'england', 'fed', 'merger', 'sharehold', 'cut', 'rise', 'day', 'bill', 'ec', 'polici', 'soviet', 'around', 'maiz', 'suppli', 'sourc', 'program', 'told', 'co', 'buy', 'growth', 'common', 'intervent', 'group', 'economi', 'petroleum', 'japanes', 'he', '1986', 'tender', 'oper', 'monetari', 'quota', 'tax', 'estim', 'pari', 'surplu', 'bushel', 'agre', 'commod', 'industri', 'quarter', 'tariff', 'domest', 'say', 'farm', 'reuter', 'shipment', 'but', 'own', 'term', 'could', 'interven', 'pay', 'level', 'world', 'stock', 'energi', 'propos', 'complet', 'european', 'fall', 'union', 'intern', 'stg', 'month', 'barley', 'commiss', 'ounc', 'purchas', 'sell', 'earn', 'season', 'port', 'it', 'ltd', 'opec', '87', 'secur', 'ad', 'bpd', 'secretari', 'outstand', 'report', 'sale', 'forecast', 'talk', 'februari', 'point', 'hous', 'subsidiari', 'shortag', 'interest', 'budget', 'meet', 'veget', 'averag', 'accord', 'gross', 'payabl', 'new', '31', 'major', 'unit', 'area', 'feder', 'gain', 'economist', '1987', 'reagan', 'committe', 'need', 'output', 'demand', 'good', 'trader', 'total', 'transact', 'mine', 'inflat', 'in', 'explor', 'gulf', 'approv', 'hold', 'plant', 'support', 'west', 'vessel', 'set', 'per', 'split', 'sorghum', 'prime', 'fund', 'bid', 'winter', 'undisclos', 'thi', 'extraordinari', 'measur', 'south', 'mark', 'ton', 'rice', 'help', 'take', 'ico', 'target', 'stabil', 'also', 'harvest', 'file', 'investor', 'we', 'ministri', 'natur', 'drill', 'repurchas', 'cash', 'sinc', 'germani', 'control', 'rose', '1985', 'discount', 'administr', 'yesterday', 'exclud', 'like', 'brazil', 'move', 'revis', 'grower', 'takeov', 'china', 'next', 'assist', 'discontinu', 'financ', 'feed', 'feet', 'negoti', 'fell', 'they', 'ussr', 'open', 'call', 'subsidi', 'way', 'januari', 'plan', 'gatt', 'commun', 'band', 'cubic', 'franc', 'drop', 'system', 'retali', 'recent', 'april', 'invest', 'spokesman', 'liquid', 'bundesbank', 'bought', 'washington', 'lend', 'show', 'disclos', 'deliveri', 'cargo', 'figur', 'deal', 'member', 'increas', 'american', 'asset', 'tokyo', 'declin', 'strike', 'oilse', 'cotton', 'expect', 'letter', 'acreag', 'white', 'weather', 'grow', 'nil', 'sea', 'presid', 'palm', 'acr', 'follow', 'time', 'firm', 'short', 'gdp', 'sign', 'develop', 'lower', 'board', 'much', 'deleg', 'council', 'contract', 'still', 'deposit', 'manag', 'must', 'current', 'congress', 'subject', 'may', 'receiv', 'discuss', '10', 'baker', 'nine', 'septemb', 'compar', 'want', 'chang', 'taiwan', 'announc', 'intent', 'semiconductor', 'consid', 'canada', 'high', 'cereal', 'aid', 'morn', 'low', 'issu', 'reduc', 'gener', 'cooper', 'ask', 'yield', 'previou', 'impos', '12', 'pressur', 'dri', 'well', '500', 'refineri', 'action', 'privat', 'hectar', 'senat', 'close', 'tanker', 'britain', 'disput', 'use', 'bag', 'futur', 'comment', 'organ', 'plc', 'far', 'decemb', 'london', '15', 'consumpt', 'refin', 'iran', 'damag', 'german', 'institut', 'pact', 'banker', 'two', 'problem', 'consum', 'texa', 'made', 'matur', 'legisl', 'session', 'larg', 'associ', 'seek', 'feedgrain', 'less', 'enhanc', 'part', 'loan', 'sterl', 'fat', 'make', 'lyng', 'howev', 'provid', 'remain', 'mani', 'grade', 'effect', '16', '86', 'french', 'earli', 'gnp', 'base', 'saudi', 'resourc', 'moscow', 'payment', 'ecu', 'analyst', 'come', 'miyazawa', 'indonesia', 'rang', '50', 'there', 'work', 'field', 'debt', 'custom', 'cover', 'account', 'agenc', 'declar', 'yeutter', 'real', 'imbal', 'hit', 'incom', 'brazilian', 'main', 'continu', 'valu', '4th', 'caus', 'tomorrow', 'ore', 'rais', 'load', 'even', 'york', 'eas', 'keep', 'one', 'reach', 'pipelin', 'see', 'transport', 'march', 'end', 'protectionist', 'among', 'think', 'adjust', 'financi', 'and', 'repres', 'accept', 'round', 'sold', '18', 'kilo', 'free', 'colombia', 'cost', 'war', 'competit', 'extern', 'push', 'indic', 'ccc', 'bbl', 'rain', 'soft', 'situat', 'whether', 'bond', 'coast', 'includ', 'tri', 'servic', 'option', 'certif', 'sector', 'rebat', 'might', 'urg', 'first', 'put', 'busi', 'daili', 'ago', 'credit', 'temporari', 'exchequ', 'held', 'subsid', 'studi', 'iraq', 'korea', 'drought', 'past', 'borrow', 'lead', 'bilater', 'narrow', 'northern', 'partner', 'spring', 'alreadi', 'ecuador', 'mean', 'afternoon', 'jame', 'canadian', 'affili', 'stimul', 'right', 'higher', 'effort', 'rule', 'back', 'decis', 'destin', 'stabilis', 'near', 'licenc', 'definit', 'bring', 'quarterli', 'long', 'quot', 'stop', 'governor', 'reject', 'warship', 'outright', 'court', '1988', 'specul', 'conserv', 'merg', '92', 'kiichi', 'latest', 'statement', 'statist', 'spend', 'bonu', 'principl', 'polit', 'drain', 'result', 'bas', 'buyout', 'condit', 'defici', 'eep', 'water', 'arabia', 'air', 'iranian', 'if', 'missil', 'weekli', 'spot', 'hope', 'de', 'meal', 'venezuela', '17', 'start', 'possibl', 'allow', 'normal', 'north', 'remark', 'manufactur', 'senior', 'india', 'warn', 'half', 'balanc', 'capit', '14', 'clayton', 'slow', 'raw', 'go', 'temperatur', 'fix', 'worth', 'properti', 'cent', 'under', 'anoth', 'forc', 'kuwait', 'sharp', 'heavi', 'protection', 'fundament', 'greec', 'dump', 'sunflow', 'littl', 'factor', 'look', 'leader', 'that', 'failur', 'africa', 'execut', 'offshor', 'add', 'requir', 'writedown', '88', 'non', 'seem', 'east', 'unfair', 'rep', 'late', '150', '100', 'contain', 'flour', 'probabl', 'richard', 'request', 'affect', 'friday', 'period', 'face', '20', 'silver', 'algeria', 'award', 'pacif', 'organis', 'charg', '25', 'sumita', 'chairman', 'protest', 'run', 'zone', 'employ', 'fail', 'toward', 'microchip', 'six', 'clear', 'miner', 'subcommitte', 'guarante', 'get', 'expir', 'fourth', 'labour', 'pretax', 'prospect', 'sever', 'earlier', 'risk', '11', 'spain', 'reduct', 'escort', 'law', '200', 'due', 'encourag', 'land', 'turnov', '30', 'comput', 'seen', 'curb', 'to', 'rio', 'step', 'crisi', '22', 'boost', 'prepar', 'rather', 'rainfal', 'freight', 'turkey', 'oppos', '1st', 'director', 'worker', 'begin', 'unchang', 'argentin', 'revenu', 'vote', 'mill', 'save', 'barrier', 'pound', 'seamen', 'within', 'congression', 'registr', 'restrict', '28', 'cargil', 'suggest', 'british', 'restat', 'can', 'attack', 'commit', 'up', 'lb', 'europ', 'present', 'river', 'rapese', 'commerc', 'limit', 'maximum', 'third', '19', 'western', 're', 'maker', 'australia', 'bean', 'southern', 'becom', 'review', 'textil', 'sentiment', 'usair', 'full', 'lawson', 'least', 'largest', 'unknown', 'enough', 'percentag', 'pik', 'overnight', 'index', 'surplus', 'gasolin', 'immedi', 'foster', 'visit', 'project', 'unlik', 'later', 'parliament', 'buyer', 'mexico', 'sanction', 'three', 'nakason', 'protect', 'chip', 'bulk', 'amount', 'inspect', 'moistur', 'provinc', 'inform', 'itali', 'crush', 'chanc', 'uruguay', '149', 'an', 'chancellor', 'concern', 'circul', 'movement', '700', '21', 'give', 'counti', 'expand', '13', 'key', 'stabl', 'ventur', 'mile', '29', 'respons', 'termin', 'not', 'fiscal', 'wednesday', 'grew', 'author', 'indian', 'sen', 'appear', 'differ', '26', 'electron', 'signal', 'prevent', 'arrang', 'decid', 'argentina', 'releas', 'bueno', 'shell', 'chicago', 'line', 'confer', 'four', 'enrol', 'improv', 'glickman', 'favour', 'commerci', 'live', 'signup', 'posit', 'resolv', 'hour', 'addit', 'previous', 'pass', 'paper', 'news', '56', 'leav', 'democrat', '77', 'exxon', 'divis', 'practic', 'enter', 'side', 'annual', 'volum', 'offic', 'weight', 'express', 'elig', 'egypt', 'block', 'oecd', 'partnership', 'june', '00', 'holder', 'dutch', 'airlin', 'grant', '48', 'aim', 'survey', 'believ', 'cold', 'underli', 'technolog', 'despit', 'resist', 'find', '23', 'interview', 'on', 'hard', 'earthquak', 'some', 'arab', 'attend', 'schedul', 'confirm', 'threaten', '34', 'advis', 'calendar', 'cyclop', 'imf', 'fuel', 'advanc', 'renew', '32', 'steadi', 'without', 'depend', 'outlook', 'equip', 'storag', 'african', 'suspend', 'although', 'capac', 'slightli', 'led', 'discoveri', 'now', 'debat', 'broad', 'food', 'link', 'bureau', 'duti', 'coars', 'carri', 'asa', 'oat', 'extend', 'netherland', 'data', '43', 'promis', 'dauster', 'emerg', 'reform', 'basic', 'portug', 'implement', 'post', '1984', 'test', 'avail', 'local', 'feb', 'process', '1990', 'trust', 'special', 'compris', 'question', 'detail', 'weak', 'coin', 'strong', 'offset', 'access', 'affair', '46', 'sown', 'summer', 'necessari', 'durum', 'diplomat', 'petrobra', 'reflect', 'began', 'region', 'combin', 'kan', 'wait', 'shift', 'home', 'volcker', 'interbank', 'jorio', 'regulatori', 'carryforward', 'night', 'packag', 'prove', 'chines', 'pre', 'belt', '800', 'ibc', 'turkish', 'each', 'rotterdam', 'view', 'number', 'via', 'miti', 'janeiro', 'retaliatori', 'case', 'deputi', 'promot', 'fee', 'm3', 'ceil', 'beet', 'press', 'amstutz', 'yasuhiro', 'depth', 'corpor', 'so', 'hand', 'ha', 'maintain', 'reason', '24', 'thou', 'dan', 'concert', 'appli', 'inject', 'jun', '36', 'kuwaiti', 'peopl', 'speak', 'processor', 'incent', 'outflow', 'attempt', 'return', 'depreci', 'convert', 'extrem', 'progress', 'light', 'certain', 'citi', 'solv', 'hole', 'fear', 'weekend', 'cane', 'avoid', '600', 'partli', 'rumor', 'spanish', 'at', '148', 'for', 'joint', 'sri', 'assay', 'lanka', '55', 'fluctuat', 'name', 'satoshi', 'arriv', 'fob', 'violat', 'parti', 'barg', 'seven', 'peak', '70', 'sunflowerse', 'handl', 'down', 'devalu', 'kind', 'impact', 'texaco', 'distribut', 'al', 'km', 'robert', 'tuesday', 'most', 'shortfal', 'form', 'elev', 'involv', 'cordoba', 'ahead', 'establish', 'kansa', 'st', 'mid', 'chase', 'altern', 'rica', 'privately', 'santa', 'trend', 'left', '38', 'moder', 'intend', 'halt', 'sugarcan', 'auction', '41', 'guilder', 'downward', 'aegean', '60', 'critic', 'power', 'order', 'greater', 'prefer', 'store', 'america', 'object', 'indirectli', 'strait', 'room', 'bp', '300', 'chinese', 'elect', 'crusher', 'ownership', 'industrialis', 'santo', '47', 'petroleo', 'provision', 'usual', 'herrington', 'hear', 'expenditur', 'flood', 'indonesian', 'alloc', 'eastern', 'potenti', 'provis', 'exercis', '73', 'minimum', 'breakdown', 'matter', 'twa', 'surg', 'seriou', 'small', 'yet', 'lloyd', 'businessmen', 'proven', 'gencorp', 'difficult', 'shipown', 'thousand', 'withdraw', '42', '27', 'distil', 'newspap', 'amend', 'basi', 'militari', 'appropri', 'applic', '68', 'tonnag', 'ralli', 'thu', 'surpris', 'korean', 'guess', 'oversea', 'settl', 'research', '78', 'introduc', 'determin', 'mad', 'warm', 'nigel', 'soon', 'snow', 'recess', 'floor', 'lowest', 'direct', 'public', 'red', 'employe', 'differenti', 'cbt', '58', 'warrant', '90', 'iraqi', 'origin', 'juli', 'taken', 'basin', '64', 'restructur', 'cocoa', 'yr', 'particip', 'approach', 'delay', 'slide', 'upward', 'zero', 'televis', 'latin', 'minn', 'feel', 'global', 'asc', 'wholli', 'mm', 'sustain', 'overal', 'five', 'pakistan', 'bread', 'second', 'inventori', 'mobil', 'embassi', 'head', 'alleg', 'tight', 'here', 'costa', 'controversi', 'australian', 'johnson', '145', 'sec', '37', '44', 'consider', 'trillion', 'independ', 'standard', 'us', 'flow', 'increasingli', 'met', 'boycott', 'deliv', 'danger', 'almost', 'arabian', 'la', '39', 'broadcast', 'shearson', 'facil', 'feasibl', 'zealand', 'act', 'broker', 'intermedi', 'former', 'speech', 'rel', 'carrier', 'along', 'neg', 'invit', 'gap', 'intens', 'huge', 'seed', 'behind', 'launch', '75', 'equival', 'aggress', 'chief', 'gave', 'float', 'monday', 'wide', '61', 'widen', 'row', 'other', 'of', 'tighten', 'benchmark', 'anti', 'california', 'stand', 'underground', 'sweeten', 'api', 'worri', 'purpos', 'solut', 'success', 'marin', 'fe', 'took', 'hormuz', 'sharpli', 'owner', 'versu', 'coordin', 'ambassador', 'eight', 'build', 'wage', 'locat', 'baldrig', 'shultz', 'smaller', 'hr', 'dilut', 'window', 'design', 'thursday', 'transfer', 'switch', 'respect', 'insur', 'anger', 'illinoi', 'refus', 'break', 'broadli', 'idea', 'especi', 'lake', 'watch', 'island', 'perform', 'tension', 'nearbi', '53', 'qtrli', 'cabl', 'collaps', 'done', 'role', 'navi', 'proce', 'reopen', 'john', 'crew', 'vice', 'complaint', 'plu', 'thought', 'doubt', 'campaign', 'parent', 'plaza', 'cif', 'inflow', 'relief', 'stanc', 'materi', 'excess', 'hutton', '82', 'undersecretari', 'levi', 'hondura', 'denmark', 'stoppag', 'adopt', '51', 'naval', '85', 'appar', 'peru', 'criteria', 'forward', 'silkworm', 'reluct', 'appreci', 'paul', 'kenya', 'piedmont', 'medium', 'dixon', 'class', 'xon', 'disast', 'achiev', 'repair', 'lobbi', 'actual', 'adher', 'courier', 'resal', 'margin', 'indirect', 'tea', 'know', 'octob', 'belgian', 'relat', 'opposit', 'quebec', '76', 'pampa', 'premium', 'widespread', 'wti', 'top', 'lot', 'soyoil', 'entr', 'belgium', 'sweet', 'signific', 'either', 'activ', 'similar', 'shelf', 'famili', 'deplet', 'sluggish', '400', 'search', 'newli', 'anyth', 'equiti', 'is', 'metal', 'degre', 'exist', 'as', 'louisiana', 'unless', 'person', 'unsolicit', 'noth', 'better', 'final', 'swap', 'inflationari', 'telecommun', 'proceed', 'headquart', 'shut', 'saw', 'obtain', 'westminst', 'stress', 'threat', 'calcul', 'dairi', 'lose', 'depress', 'branch', 'flexibl', '63', 'all', 'longer', 'sail', 'republican', 'bullish', 'charter', 'jan', 'exampl', 'instal', 'came', '45', 'quantiti', 'lift', 'flag', 'bale', '480', 'wet', 'prompt', 'with', 'aggreg', 'continent', 'lira', '147', 'quickli', '40', 'no', 'restor', 'tough', '81', 'royal', 'occur', 'decreas', 'iowa', 'poland', 'chemic', 'gmt', 'leverag', 'philippin', 'out', 'tehran', 'apr', 'pod', 'cabinet', 'prioriti', 'resum', 'confid', 'moment', 'particularli', 'paid', 'recoveri', 'ferri', 'goal', 'draft', 'defenc', 'across', 'repli', 'sour', '54', 'midwest', '1989', 'again', 'abroad', 'lawmak', 'sow', 'though', 'nato', 'colombian', 'july', 'idl', 'found', 'agenda', 'ban', 'discov', 'ontario', 'alon', 'brought', 'regul', 'over', 'retain', 'bangladesh', 'em', 'given', 'further', 'el', 'play', 'ab', '84', 'bullion', 'will', 'creat', 'contact', 'extens', 'recommend', 'middl', 'defin', 'poehl', 'life', 'panel', 'item', 'outsid', 'accus', 'massiv', 'crown', 'ocean', 'fructos', 'syrup', '350', 'separ', 'purol', 'august', 'leas', 'formal', 'bonn', 'permit', 'steadili', 'irrig', 'lo', 'deni', 'minu', 'what', 'daniel', 'fair', 'poor', 'replac', 'liberalis', 'publish', 'parliamentari', 'monthli', 'citibank', 'spread', 'mideast', 'specif', 'clearli', 'cuba', 'serious', 'video', 'favor', 'join', 'station', 'brother', 'old', 'tonight', 'emir', '1981', 'rupe', 'hernandez', 'fresh', 'recov', 'unemploy', 'singapor', 'dlrs', 'tradit', 'coconut', 'fnv', 'yellow', 'pl', 'instead', 'tran', 'consolid', 'edibl', 'intersect', 'merchant', 'nearli', '80', 'boschwitz', 'identifi', 'trip', 'manhattan', 'usx', 'refer', 'maritim', 'lack', 'jungl', 'undermin', 'rig', 'san', 'fairli', 'great', 'radio', 'goe', 'grisanti', 'ohio', 'waterway', 'peke', 'lehman', 'predict', 'canal', '05', 'modest', 'portland', 'contribut', 'front', '04', 'tell', 'realli', 'initi', 'olein', 'poll', 'allegheni', 'pat', 'attract', 'pric', 'correct', 'interior', 'compromis', 'varieti', '120', 'calif', 'send', 'sought', 'difficulti', 'appeal', 'multilater', 'argu', '98', 'bearish', 'whole', 'suffer', 'asian', '71', 'gradual', '900', 'decoupl', 'sweden', 'saturday', 'retail', 'ag', 'farmland', 'greek', 'worldwid', 'gilt', 'kernel', 'strateg', 'navig', 'react', 'consult', 'regard', '1980', 'diseas', 'oilfield', 'hostil', 'malcolm', 'list', 'qatar', 'construct', 'swing', '123', 'persian', 'malaysian', 'extract', 'aviat', 'stc', 'redund', 'georg', 'thailand', 'est', 'defend', 'angel', 'dec', 'novemb', 'elsewher', 'onli', 'spur', 'best', 'shel', 'pend', '65', '03', 'justic', 'insect', 'subproduct', 'malaysia', 'pest', '33', 'suit', 'green', 'them', 'fulli', 'endors', 'chemlawn', 'delhi', 'virtual', 'sensit', 'compens', '79', '1982', 'contrast', 'dwt', 'hurt', 'rival', 'rapid', 'reli', 'basket', 'adequ', 'liber', 'mt', 'rest', 'morgan', '72', 'pdvsa', 'understand', 'resid', 'bad', 'drastic', 'mississippi', 'kong', 'evalu', 'section', 'conflict', '93', '101', 'perhap', 'hong', 'alli', 'defens', 'cross', 'influenc', 'technic', 'persuad', 'wholly', 'norway', 'mar', 'grown', 'respond', 'rd', 'specialist', 'expans', 'empti', 'cheap', 'util', 'trigger', 've', 'or', 'mouth', 'black', 'republ', 'more', 'scope', 'karl', 'balao', 'alvit', 'friction', 'strongli', 'soar', '89', 'worst', 'stood', 'strength', 'guatemala', 'vital', 'william', 'wall', 'abl', 'percept', 'otto', 'conduct', 'ten', 'subordin', 'receipt', 'aircraft', 'motor', 'brokerag', 'beef', 'autumn', 'draw', 'histor', '62', 'enterpris', 'dominican', 'regist', 'convinc', '1979', '57', 'jose', 'pump', '137', 'afg', 'fleet', 'countertrad', 'cyacq', 'fire', 'highest', 'platinum', 'southeast', '750', 'broke', 'categori', 'mortgag', 'harbour', 'cd', 'cwt', 'toledo', 'wagner', 'bennett', 'boston', 'risen', 'spark', 'fallen', 'ground', 'benefit', 'nicaragua', 'street', 'away', 'turn', 'venezuelan', 'weaken', 'scheme', 'singl', '35', 'tobacco', 'reneg', 'bidder', 'hugh', 'summari', 'borg', 'kept', 'taft', 'opportun', 'pattern', 'alaska', 'soil', 'managua', 'prevail', 'thing', 'thai', 'expens', 'serv', 'shipbuild', 'weaker', 'debentur', 'await', 'previously', 'easier', 'advisor', 'place', 'carl', 'mutual', 'sept', 'jointli', 'disappear', 'bloc', '67', 'coverag', 'compet', 'sent', 'ortner', 'rubber', 'tex', 'insist', 'rudi', 'deterior', 'readi', 'resolut', 'bear', 'injur', 'bay', 'stick', 'copper', 'steel', 'meter', 'known', 'seller', 'upper', 'tool', 'disrupt', 'clercq', 'willi', 'rostenkowski', 'bu', 'restraint', 'slight', 'network', 'supermarket', 'troop', 'houston', 'let', 'asia', 'brown', 'bancorp', 'intellig', 'swiss', 'fla', 'bracket', 'chaco', 'corrient', 'formosa', 'mision', 'october', 'content', 'care', 'midnight', 'qualiti', 'rand', 'cautiou', 'onic', 'toler', '119', 'linse', 'address', 'northwest', 'm0', 'creditor', 'sure', 'date', 'sceptic', 'missouri', '250', '103', 'caution', 'foot', 'fujitsu', 'fast', 'undertak', 'sarney', 'wind', 'strengthen', 'electr', 'pioneer', 'fernando', '49', 'undervalu', 'edg', 'commission', 'claim', 'unilater', 'chrysler', 'estat', 'willing', 'repay', 'roll', 'reaction', 'stronger', 'morocco']\n",
            "         vs        ct       shr       net     said  wheat       rev  tonn  \\\n",
            "0  0.293739  0.187227  0.124981  0.104972  0.00000    0.0  0.167840   0.0   \n",
            "1  0.182614  0.058199  0.155399  0.130520  0.00000    0.0  0.104344   0.0   \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.01082    0.0  0.000000   0.0   \n",
            "3  0.274573  0.000000  0.116826  0.098123  0.00000    0.0  0.156888   0.0   \n",
            "4  0.313327  0.000000  0.159979  0.201550  0.00000    0.0  0.107419   0.0   \n",
            "\n",
            "       bank  oil  ...  claim  unilater  chrysler  estat  willing  repay  roll  \\\n",
            "0  0.000000  0.0  ...    0.0       0.0       0.0    0.0      0.0    0.0   0.0   \n",
            "1  0.000000  0.0  ...    0.0       0.0       0.0    0.0      0.0    0.0   0.0   \n",
            "2  0.169403  0.0  ...    0.0       0.0       0.0    0.0      0.0    0.0   0.0   \n",
            "3  0.000000  0.0  ...    0.0       0.0       0.0    0.0      0.0    0.0   0.0   \n",
            "4  0.000000  0.0  ...    0.0       0.0       0.0    0.0      0.0    0.0   0.0   \n",
            "\n",
            "   reaction  stronger  morocco  \n",
            "0       0.0       0.0      0.0  \n",
            "1       0.0       0.0      0.0  \n",
            "2       0.0       0.0      0.0  \n",
            "3       0.0       0.0      0.0  \n",
            "4       0.0       0.0      0.0  \n",
            "\n",
            "[5 rows x 2000 columns]\n",
            "         vs        ct       shr       net      said  wheat  rev  tonn  bank  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000  0.063970    0.0  0.0   0.0   0.0   \n",
            "1  0.224528  0.152483  0.081468  0.074158  0.000000    0.0  0.0   0.0   0.0   \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.051028    0.0  0.0   0.0   0.0   \n",
            "3  0.000000  0.000000  0.000000  0.000000  0.072549    0.0  0.0   0.0   0.0   \n",
            "4  0.329631  0.223862  0.119605  0.108872  0.000000    0.0  0.0   0.0   0.0   \n",
            "\n",
            "        oil  ...  claim  unilater  chrysler  estat  willing  repay  roll  \\\n",
            "0  0.000000  ...    0.0       0.0       0.0    0.0      0.0    0.0   0.0   \n",
            "1  0.000000  ...    0.0       0.0       0.0    0.0      0.0    0.0   0.0   \n",
            "2  0.106282  ...    0.0       0.0       0.0    0.0      0.0    0.0   0.0   \n",
            "3  0.000000  ...    0.0       0.0       0.0    0.0      0.0    0.0   0.0   \n",
            "4  0.000000  ...    0.0       0.0       0.0    0.0      0.0    0.0   0.0   \n",
            "\n",
            "   reaction  stronger  morocco  \n",
            "0       0.0       0.0      0.0  \n",
            "1       0.0       0.0      0.0  \n",
            "2       0.0       0.0      0.0  \n",
            "3       0.0       0.0      0.0  \n",
            "4       0.0       0.0      0.0  \n",
            "\n",
            "[5 rows x 2000 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right above we can see a dataframe consisting of all the training sample with the tfidf representation. As we can see for each row we have a set of values in function of the most discriminant features."
      ],
      "metadata": {
        "id": "O-I7DO01e5FW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification and Results\n",
        "Below are reported the functions related to the training and testing of some classifiers and the evalution of their performances.\n",
        "\n",
        "The performances function takes the true labels and the predicted ones, and compute some metrics to evalutate the performances of the classifier.\n",
        "It computes:\n",
        "- the accuracy as the number of correctly classified samples divided by the total number of samples\n",
        "- the precision, which is a metric that focuses on the positive predictions made by the classifier. It is the proportion of true positive predictions (correctly predicted positive samples) out of all the positive predictions (both true positives and false positives).\n",
        "- the recall, also known as sensitivity or true positive rate, measures the ability of the classifier to correctly identify positive samples (true positives) out of all the samples that are actually positive (true positives + false negatives).\n",
        "- The F_beta score is a balanced metric that combines precision and recall using a parameter beta. It allows us to control the emphasis on either precision or recall based on the value of beta. The F1 score is a special case of the F_beta score when beta is set to 1, giving equal importance to precision and recall. The F_beta score is calculated as follows:\n",
        "$$F_Î² = (1+\\beta^2) â Precision \\cdot Recall \\over Î²^2 \\cdot Precision + Recall $$\n",
        "\n",
        "Hence, setting Î² = 1 gives the F1 score, which is commonly used when we want a balance between precision and recall.\n",
        "\n",
        "In the code, average='macro' is used for precision, recall, and F_beta score calculations. This means that the metrics are calculated for each class, and the macro-average is taken to get an overall score across all classes, giving equal weight to each class regardless of its size."
      ],
      "metadata": {
        "id": "Abl96jsfBQr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score, precision_score, recall_score\n",
        "\n",
        "def performances(y_test, y_pred):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    recall = recall_score(y_test, y_pred, average='macro', zero_division=0.0)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall\", recall)\n",
        "    print(\"F_beta score:\", fbeta_score(y_test, y_pred, average='macro', beta=1))\n",
        "\n"
      ],
      "metadata": {
        "id": "fdQIYWvEBP0V"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classify function takes the training and testing data X_train, y_train, X_test, and y_test as input. Then the code creates an instance of the SVC and MLP classifier and then wraps it using OneVsRestClassifier, which allows multi-label classification. The model is trained on the training data (X_train, y_train) and tested on the testing data (X_test). The performance metrics are then computed and printed using the performances function.\n",
        "\n",
        "The OneVsRestClassifier implements the One-vs-the-rest (OvR) multiclass strategy. Also known as one-vs-all strategy, this method involves training a separate classifier for each class. Each classifier is designed to distinguish its corresponding class from all the other classes. Therefore, OneVsRestClassifier can be used for multilabel classification tasks. To utilize this feature, the target labels should be presented as a 2D binary matrix, where [i, j] == 1 indicates the presence of label j in sample i (as in our case). When used for multilabel classification, the estimator employs the binary relevance method, training one binary classifier independently for each label."
      ],
      "metadata": {
        "id": "QZv0l_SLbxKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "def classify(X_train, y_train, X_test, y_test):\n",
        "\n",
        "    ####### SVC #########\n",
        "    print(\"Training and testing with a SVC classifier\")\n",
        "    clf1 = SVC()\n",
        "    clfs1 = OneVsRestClassifier(clf1, verbose=51).fit(X_train, y_train)\n",
        "    y_pred = clfs1.predict(X_test)\n",
        "    print(\"SVC performances\")\n",
        "    performances(y_test, y_pred)\n",
        "    ####### MLP #########\n",
        "    print(\"Training and testing with a MLP classifier\")\n",
        "    clf2 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
        "    clfs2 = OneVsRestClassifier(clf2, verbose=51).fit(X_train, y_train)\n",
        "    y_pred = clfs2.predict(X_test)\n",
        "    print(\"MLP performances\")\n",
        "    performances(y_test, y_pred)\n",
        "\n",
        "    print(\"y_test:\\n\", y_test[5:15])\n",
        "    print(\"y_pred:\\n\", y_pred[5:15])\n"
      ],
      "metadata": {
        "id": "DDnIui3_BX7X"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBGhJN9zBdv0",
        "outputId": "67cbb2f6-5c1d-41d7-cbcd-f07f63d5e629"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and testing with a SVC classifier\n",
            "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:   27.2s\n",
            "[Parallel(n_jobs=1)]: Done   2 tasks      | elapsed:   56.0s\n",
            "[Parallel(n_jobs=1)]: Done   3 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=1)]: Done   4 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=1)]: Done   5 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=1)]: Done   6 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=1)]: Done   7 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=1)]: Done   8 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=1)]: Done   9 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=1)]: Done  10 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=1)]: Done  11 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=1)]: Done  12 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=1)]: Done  13 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=1)]: Done  14 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=1)]: Done  15 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=1)]: Done  16 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:  3.9min\n",
            "[Parallel(n_jobs=1)]: Done  18 tasks      | elapsed:  4.0min\n",
            "[Parallel(n_jobs=1)]: Done  19 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=1)]: Done  20 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=1)]: Done  20 tasks      | elapsed:  4.3min\n",
            "SVC performances\n",
            "Accuracy: 0.8363435311463324\n",
            "Precision: 0.9268807256863203\n",
            "Recall 0.6648890435915871\n",
            "F_beta score: 0.7630048922904435\n",
            "Training and testing with a MLP classifier\n",
            "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:   31.8s\n",
            "[Parallel(n_jobs=1)]: Done   2 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=1)]: Done   3 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=1)]: Done   4 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=1)]: Done   5 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=1)]: Done   6 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=1)]: Done   7 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=1)]: Done   8 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=1)]: Done   9 tasks      | elapsed:  4.9min\n",
            "[Parallel(n_jobs=1)]: Done  10 tasks      | elapsed:  5.3min\n",
            "[Parallel(n_jobs=1)]: Done  11 tasks      | elapsed:  6.0min\n",
            "[Parallel(n_jobs=1)]: Done  12 tasks      | elapsed:  6.4min\n",
            "[Parallel(n_jobs=1)]: Done  13 tasks      | elapsed:  6.8min\n",
            "[Parallel(n_jobs=1)]: Done  14 tasks      | elapsed:  7.2min\n",
            "[Parallel(n_jobs=1)]: Done  15 tasks      | elapsed:  7.5min\n",
            "[Parallel(n_jobs=1)]: Done  16 tasks      | elapsed:  7.9min\n",
            "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:  8.3min\n",
            "[Parallel(n_jobs=1)]: Done  18 tasks      | elapsed:  8.6min\n",
            "[Parallel(n_jobs=1)]: Done  19 tasks      | elapsed:  9.0min\n",
            "[Parallel(n_jobs=1)]: Done  20 tasks      | elapsed:  9.5min\n",
            "[Parallel(n_jobs=1)]: Done  20 tasks      | elapsed:  9.5min\n",
            "MLP performances\n",
            "Accuracy: 0.825654257279764\n",
            "Precision: 0.8662570407877563\n",
            "Recall 0.6994155159282671\n",
            "F_beta score: 0.7644215279985977\n",
            "y_test:\n",
            " [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "y_pred:\n",
            " [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the results above, the classifiers behave in the same way by presenting comparable presentations. Finally, the predictions of 10 samples from the last classifier compared to the true labels of the same samples are printed, highlighting how the classifier is performing well.\n",
        "\n",
        "As a final consideration, we encourage changing the number of classes to be considered, as well as the number of discriminating features, to see how the overall performance of the classifiers varies.\n",
        "\n",
        "By reducing the number of classes to 5 for instance, performance rises above 95 % due to the strong presence of samples with those labels, but with a strongly low number of multi labelled samples.  This is due to the non-presence of samples whose labels are among the 5 most common.\n",
        "Moreover, it can be shown that with an increase (above approximately 1500) in the number of features to represent the samples with, performance goes down due to the inclusion of features that are too poorly discriminating."
      ],
      "metadata": {
        "id": "6vDXzQTNf8kP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ex2NAJ8ailuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}